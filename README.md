# Deep Q-Network for Atari DemonAttack

PyTorch implementation of Deep Q-Networks (DQN) and its variants for mastering the Atari game DemonAttack.

---

## ğŸ“‹ Problem Description

### DemonAttack Environment

**DemonAttack** is a classic Atari 2600 game where the player defends against waves of demons on the ice planet Krybor. The objective is to survive enemy attacks and maximize score by destroying demons.

**Environment Specifications:**
- **Action Space**: `Discrete(6)` - NOOP, FIRE, LEFT, RIGHT, LEFTFIRE, RIGHTFIRE
- **Observation Space**: `Box(0, 255, (210, 160, 3), uint8)` RGB frames
- **Challenge**: Requires precise timing, spatial awareness, and strategic decision-making

**Game Mechanics:**
- Player starts with 3 reserve bunkers (can increase up to 6)
- Surviving each wave without damage grants an additional bunker
- Enemy hit destroys one bunker; losing all bunkers ends the game
- Score increases by destroying different types of demons

### Research Challenge

The goal is to train a deep reinforcement learning agent that can:
1. Learn effective policies from raw pixel inputs
2. Achieve human-level or superhuman performance
3. Generalize across different enemy patterns and game states

**Baseline Benchmarks:**
- Random agent: ~50 points
- Human average: ~1,971 points
- Human expert: ~3,401 points
- Published DQN: 3,000-9,000 points
- Rainbow DQN: 100,000+ points

---

## ğŸ§  Approach & Model Architecture

This project implements multiple DQN variants with increasing sophistication:

### 1. Baseline DQN (Mnih et al., Nature 2015)

**Architecture:**
```
Input: [batch, 4, 84, 84] (4 stacked grayscale frames)
  â†“
Conv2d(4â†’32, kernel=8, stride=4) + ReLU
  â†“
Conv2d(32â†’64, kernel=4, stride=2) + ReLU
  â†“
Conv2d(64â†’64, kernel=3, stride=1) + ReLU
  â†“
Flatten â†’ [batch, 3136]
  â†“
Linear(3136â†’512) + ReLU
  â†“
Linear(512â†’n_actions) â†’ Q-values
```

**Key Techniques:**
- **Experience Replay**: Store and sample past transitions to break temporal correlation
- **Target Network**: Stabilize training with slowly updated Q-value targets
- **Îµ-greedy Exploration**: Balance exploration and exploitation
- **Frame Stacking**: Use 4 consecutive frames to capture motion information

### 2. Dueling DQN (Wang et al., ICML 2016)

Separates Q-value computation into two streams:

```
Shared Features (Conv layers)
  â†“
  â”œâ”€â†’ Value Stream: V(s) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚                                â†“
  â””â”€â†’ Advantage Stream: A(s,a) â”€â”€â”€â†’ Q(s,a) = V(s) + [A(s,a) - mean(A)]
```

**Advantages:**
- Better learning of state values independent of actions
- More stable training for environments with many redundant actions
- Improved performance on games requiring precise action selection

### 3. Noisy Dueling DQN (Fortunato et al., 2017)

Replaces Îµ-greedy with learned parametric noise in network weights:

**NoisyLinear Layer:**
```
W = Î¼_w + Ïƒ_w âŠ™ Îµ_w  (training)
W = Î¼_w              (evaluation)
```

Where:
- `Î¼_w`: Learnable mean weights
- `Ïƒ_w`: Learnable noise scale
- `Îµ_w`: Factorized Gaussian noise

**Benefits:**
- Automatic exploration without manual Îµ scheduling
- Better late-stage exploration for complex strategies
- State-dependent exploration (different noise per state)

### Additional Improvements

**Double DQN** (van Hasselt et al., 2015):
- Reduces Q-value overestimation
- Uses online network for action selection, target network for evaluation

**Prioritized Experience Replay** (Schaul et al., 2015):
- Samples important transitions more frequently
- Weighted importance sampling for unbiased gradient updates

---

## ğŸš€ Quick Start

### Installation

```bash
# Using uv (recommended)
uv sync

# Or using pip
pip install -e .
```

### Training

```bash
# Baseline DQN (500k steps, ~2-4 hours)
uv run python -m dqn_demon_attack.scripts.train_yaml --config configs/baseline_dqn.yaml

# Dueling DQN (2M steps, ~8-12 hours, recommended)
uv run python -m dqn_demon_attack.scripts.train_yaml --config configs/dueling_dqn.yaml

# Noisy Dueling DQN (3M steps, ~15-20 hours, best performance)
uv run python -m dqn_demon_attack.scripts.train_yaml --config configs/noisy_dueling_dqn.yaml
```

### Evaluation

```bash
# Evaluate trained model
python -m dqn_demon_attack.scripts.eval --ckpt runs/dueling_dqn/checkpoints/final.pt --episodes 10

# Watch agent play
python -m dqn_demon_attack.scripts.watch --ckpt runs/dueling_dqn/checkpoints/final.pt --mode human
```

### Visualization

```bash
# Plot training curves
python -m dqn_demon_attack.utils.viz --log runs/dueling_dqn/train_log.csv
```

---

## âš™ï¸ Configuration

### Available Configurations

| Config | Model | Steps | Training Time | Target Score |
|--------|-------|-------|---------------|--------------|
| `baseline_dqn.yaml` | DQN | 500k | 2-4h | ~150 |
| `dueling_dqn.yaml` â­ | DuelingDQN | 2M | 8-12h | ~400-600 |
| `noisy_dueling_dqn.yaml` ğŸ† | NoisyDuelingDQN | 3M | 15-20h | ~800-1000+ |

### Configuration Template

```yaml
# Experiment Settings
exp_name: my_experiment
total_steps: 2000000
eval_every: 100000
seed: 42
device: cuda  # or 'cpu'

# Model Settings
model_type: DuelingDQN  # DQN | DuelingDQN | NoisyDQN | NoisyDuelingDQN
reward_mode: clip       # clip | scaled | raw

# Training Hyperparameters
gamma: 0.99             # Discount factor
lr: 0.00025             # Learning rate
batch_size: 64          # Batch size
replay_size: 200000     # Replay buffer capacity
warmup: 20000           # Random warmup steps
target_update_freq: 2000  # Target network update frequency
grad_clip: 10.0         # Gradient clipping threshold

# Exploration (ignored for Noisy Networks)
eps_start: 1.0
eps_end: 0.01
eps_decay_steps: 1000000

# Advanced Features
use_double_dqn: true
use_prioritized_replay: false
```

### Key Hyperparameters

**Learning Rate (`lr`)**:
- Baseline: 1e-4
- Recommended: 2.5e-4
- Higher values risk instability

**Replay Buffer (`replay_size`)**:
- Minimum: 50k
- Recommended: 200k
- Maximum: 500k (for best models)

**Batch Size (`batch_size`)**:
- Small (32): Faster updates, higher variance
- Large (128): More stable, slower updates
- Recommended: 64

**Target Update Frequency (`target_update_freq`)**:
- Too frequent (<1k): Training instability
- Too rare (>10k): Slow adaptation
- Recommended: 2k-5k steps

---

## ğŸ“Š Expected Results

### Performance Comparison

| Method | Score | Training Steps | Time (GPU) | Improvement |
|--------|-------|----------------|------------|-------------|
| Random | ~50 | - | - | Baseline |
| Baseline DQN | ~150 | 500k | 2-4h | 3x |
| Dueling DQN | ~400-600 | 2M | 8-12h | 8-12x |
| Noisy Dueling DQN | ~800-1000+ | 3M | 15-20h | 16-20x |
| Human Average | ~1,971 | - | - | 39x |
| Human Expert | ~3,401 | - | - | 68x |

### Training Dynamics

**Baseline DQN**:
- Initial phase (0-100k): Random exploration, score ~40-60
- Learning phase (100k-300k): Policy improvement, score ~80-120
- Convergence (300k-500k): Stable performance, score ~130-150
- High variance (Â±90 points)

**Dueling DQN**:
- Warmup (0-100k): Initial exploration, score ~50-80
- Rapid learning (100k-500k): Major improvements, score ~150-300
- Refinement (500k-2M): Strategy optimization, score ~400-600
- Medium variance (Â±120 points)

**Noisy Dueling DQN**:
- Early exploration (0-200k): Noisy network learning, score ~60-100
- Stable learning (200k-1M): Consistent improvements, score ~200-500
- Advanced strategies (1M-3M): Complex behaviors, score ~800-1000+
- Lower variance (Â±150 points)

---

## ğŸ“ Project Structure

```
RL-DemonAttack/
â”œâ”€â”€ configs/                      # Configuration files
â”‚   â”œâ”€â”€ baseline_dqn.yaml        # Nature DQN baseline
â”‚   â”œâ”€â”€ dueling_dqn.yaml         # Dueling architecture (recommended)
â”‚   â””â”€â”€ noisy_dueling_dqn.yaml   # Best performance
â”‚
â”œâ”€â”€ src/dqn_demon_attack/
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ models.py            # DQN architectures
â”‚   â”‚   â””â”€â”€ replay.py            # Experience replay buffers
â”‚   â”œâ”€â”€ envs/
â”‚   â”‚   â””â”€â”€ wrappers.py          # Environment preprocessing
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ config.py            # Configuration management
â”‚   â”‚   â”œâ”€â”€ logger.py            # Training logging
â”‚   â”‚   â””â”€â”€ viz.py               # Visualization tools
â”‚   â””â”€â”€ scripts/
â”‚       â”œâ”€â”€ train_yaml.py        # Training script
â”‚       â”œâ”€â”€ eval.py              # Evaluation script
â”‚       â””â”€â”€ watch.py             # Visualization script
â”‚
â”œâ”€â”€ runs/                         # Training outputs
â”‚   â””â”€â”€ <exp_name>/
â”‚       â”œâ”€â”€ config.yaml          # Saved configuration
â”‚       â”œâ”€â”€ train_log.csv        # Training metrics
â”‚       â””â”€â”€ checkpoints/         # Model checkpoints
â”‚
â”œâ”€â”€ pyproject.toml               # Dependencies
â””â”€â”€ README.md                    # This file
```

---

## ğŸ”¬ Implementation Details

### Preprocessing Pipeline

1. **Grayscale Conversion**: RGB â†’ grayscale (210Ã—160Ã—3 â†’ 210Ã—160)
2. **Resizing**: 210Ã—160 â†’ 84Ã—84 (bilinear interpolation)
3. **Frame Stacking**: Stack 4 consecutive frames â†’ 4Ã—84Ã—84
4. **Normalization**: [0, 255] â†’ [0.0, 1.0]
5. **Reward Clipping**: Clip rewards to [-1, 0, +1]

### Training Algorithm

```python
1. Initialize replay buffer D, Q-network Q(Î¸), target network QÌ‚(Î¸â»)
2. For each episode:
    a. Reset environment, get initial state sâ‚€
    b. For each timestep:
        - Select action: a = argmax Q(s,a) with prob. 1-Îµ, else random
        - Execute action a, observe reward r, next state s'
        - Store transition (s, a, r, s', done) in D
        - Sample minibatch from D
        - Compute target: y = r + Î³Â·max QÌ‚(s',a')
        - Update Q-network: minimize (Q(s,a) - y)Â²
        - Periodically update target network: Î¸â» â† Î¸
```

### Loss Function

**Baseline/Dueling DQN**:
```
L(Î¸) = ğ”¼[(r + Î³Â·max QÌ‚(s',a') - Q(s,a))Â²]
```

**Double DQN**:
```
L(Î¸) = ğ”¼[(r + Î³Â·QÌ‚(s', argmax Q(s',a')) - Q(s,a))Â²]
```

**Prioritized Replay**:
```
L(Î¸) = ğ”¼[w_i Â· (r + Î³Â·QÌ‚(s',a') - Q(s,a))Â²]
where w_i = (N Â· P(i))â»áµ  (importance weight)
```

---

## ğŸ› ï¸ Troubleshooting

### Out of Memory
- Reduce `batch_size` to 32
- Reduce `replay_size` to 100k
- Close other GPU applications

### Low Performance
- Train longer (`total_steps` â†’ 2M+)
- Use better architecture (`DuelingDQN` or `NoisyDuelingDQN`)
- Increase replay buffer (`replay_size` â†’ 200k+)
- Adjust learning rate (`lr` â†’ 2.5e-4)

### Training Instability
- Reduce learning rate (`lr` â†’ 1e-4)
- Increase batch size (`batch_size` â†’ 128)
- Enable gradient clipping (`grad_clip: 10.0`)
- Increase target update frequency (`target_update_freq` â†’ 5k)

---

## ğŸ“š References

### Core Papers

1. **DQN**: Mnih et al. (2015). "Human-level control through deep reinforcement learning." *Nature*.
2. **Double DQN**: van Hasselt et al. (2015). "Deep Reinforcement Learning with Double Q-learning." *AAAI*.
3. **Dueling DQN**: Wang et al. (2016). "Dueling Network Architectures for Deep Reinforcement Learning." *ICML*.
4. **Prioritized Replay**: Schaul et al. (2015). "Prioritized Experience Replay." *ICLR*.
5. **Noisy Networks**: Fortunato et al. (2017). "Noisy Networks for Exploration." *ICLR*.
6. **Rainbow**: Hessel et al. (2018). "Rainbow: Combining Improvements in Deep Reinforcement Learning." *AAAI*.

### Implementation

- [PyTorch](https://pytorch.org/) - Deep learning framework
- [Gymnasium](https://gymnasium.farama.org/) - Atari game environments
- [ALE](https://github.com/mgbellemare/Arcade-Learning-Environment) - Arcade Learning Environment

---

## ğŸ¤ Contributing

See [CONTRIBUTING.md](CONTRIBUTING.md) for contribution guidelines.

---

## ğŸ“„ License

This project is for educational and research purposes.

---

## ğŸ™ Acknowledgments

This implementation is based on foundational work by DeepMind and the reinforcement learning research community. Special thanks to the authors of the papers listed in the references section.
