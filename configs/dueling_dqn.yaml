# Dueling DQN Configuration
# Dueling architecture (Wang et al., 2016) with optimized hyperparameters

# Experiment Settings
exp_name: dueling_dqn
total_steps: 2000000
eval_every: 100000
seed: 42
device: cuda
start_from: null

# Environment Settings
terminal_on_life_loss: true
screen_size: 84
frame_stack: 4

# Model Settings
model_type: DuelingDQN  # Using Dueling architecture
reward_mode: clip

# Training Hyperparameters
gamma: 0.99
lr: 0.0000625  # Optimized learning rate for RMSprop
lr_final: 0.000025  # Gentle linear decay target
lr_decay_steps: 2000000
batch_size: 32  # Larger batch for stability
replay_size: 150000  # Large replay buffer within 16GB RAM
warmup: 50000
target_update_freq: 4000  # Faster target updates to reduce drift
grad_clip: 5.0

# Exploration Settings
eps_start: 1.0
eps_end: 0.01  # Lower final epsilon
eps_decay_steps: 1000000

# Advanced Features
use_double_dqn: true
use_prioritized_replay: false
per_alpha: 0.6
per_beta_start: 0.4
per_beta_frames: 100000

# Reward Shaping
use_reward_shaping: false
use_life_penalty: false
life_penalty: -1.0
use_streak_bonus: false
streak_window: 12
streak_bonus: 0.1

# Evaluation
eval_episodes: 20  # More robust evaluation statistics
